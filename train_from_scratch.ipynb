{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efd76cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "import json \n",
    "\n",
    "import math \n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "import numpy as np \n",
    "\n",
    "import torch \n",
    "from torch import nn, Tensor\n",
    "from torch.nn import Linear, Transformer\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import sentencepiece as spm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2071bcee",
   "metadata": {},
   "source": [
    "# Data: EN-DE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5f95c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.train(input=\"data/raw_data/tokenizer/wmt2014_train.en\", \n",
    "                               model_prefix='data/tokenizer/en_tokenizer', vocab_size=30_000, \n",
    "                               character_coverage=1.0, model_type='bpe',\n",
    "                               user_defined_symbols=['<s>', '</s>', '<pad>', '<mask>'], \n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2b17ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer = T.SentencePieceTokenizer(\"./data/tokenizer/en_tokenizer.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69f08ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_iterator(lang):\n",
    "    lst = []\n",
    "    with open(f\"./data/tokenizer/{lang}_tokenizer.vocab\") as f: \n",
    "        rd = csv.reader(f, delimiter='\\t')\n",
    "        for row in rd: \n",
    "            lst.append(row[0])\n",
    "    return lst\n",
    "\n",
    "en_vocab = build_vocab_from_iterator(vocab_iterator(\"en\"),) #specials=[\"<unk>\", \"<s>\", \"</s>\"])\n",
    "\n",
    "len(en_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9f25ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"Hello, my name is Zhangir\", \"Hello! My name is Gru.\"]\n",
    "out = en_tokenizer(text)\n",
    "vcb = T.VocabTransform(en_vocab)\n",
    "outout = vcb(out)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0f91e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer = PreTrainedTokenizerFast(\"./data/tokenizer/en_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f352c969",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[PAD]\"])\n",
    "en_tok = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "en_tok.pre_tokenizer = Whitespace() \n",
    "files = [\"data/raw_data/tokenizer/wmt2014_train.de\"]\n",
    "de_tok.train(files, trainer)\n",
    "\n",
    "de_tokenizer = PreTrainedTokenizerFast(tokenizer_object=en_tok)\n",
    "de_tokenizer.save_pretrained(\"./data/tokenizer/en_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feff428",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "de_tok = BpeTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[EOS]\", \"[BOS]\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbeda70",
   "metadata": {},
   "source": [
    "First, let's get a feel for the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a52cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df90f439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using this for convenience just for now\n",
    "en_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "de_tokenizer = BertTokenizer.from_pretrained(\"bert-base-german-cased\")\n",
    "de_tokenizer.bos_token = \"[BOS]\"\n",
    "de_tokenizer.eos_token = \"[EOS]\"\n",
    "print(en_tokenizer)\n",
    "print(de_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f7c6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/raw_data/mmt_wmt17_train.en\") as f: \n",
    "    en_data = [x.strip() for x in f.readlines()]\n",
    "\n",
    "with open(\"data/raw_data/mmt_wmt17_train.de\") as f: \n",
    "    # we're going to use sep as bos and \n",
    "    de_data = [de_tokenizer.bos_token + x.strip() + de_tokenizer.eos_token for x in f.readlines()]\n",
    "    \n",
    "en_de_data = [(x, y) for x,y in zip(en_data, de_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad0f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(x) for x in en_de_data[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63d3a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll be using torch.utils.DataLoader a lot \n",
    "loader = DataLoader(en_de_data[:5], batch_size=3)\n",
    "\n",
    "for x in loader:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95036f49",
   "metadata": {},
   "source": [
    "en_lens = [len(x) for x in en_tokenizer(en_data)['input_ids']]\n",
    "plt.hist(en_lens)\n",
    "plt.title(\"Lengths of input sequences\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df16bd5c",
   "metadata": {},
   "source": [
    "de_lens = [len(x) for x in de_tokenizer(de_data)['input_ids']]\n",
    "plt.hist(de_lens)\n",
    "plt.title(\"Lengths of output sequences\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8ee0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on our data, we declare the following parameters: \n",
    "\n",
    "SRC_SEQ_LEN = 55\n",
    "TGT_SEQ_LEN = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fb7d5e",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f4f584",
   "metadata": {},
   "source": [
    "First, let's define how we're doing positional encodings. We're going to use learned positional encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39afb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module): \n",
    "    def __init__(self, vocab_size, d_embedding, max_seq_len): \n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, d_embedding)\n",
    "        self.pos_embeddings = nn.Embedding(max_seq_len, d_embedding)\n",
    "    \n",
    "    def forward(self, x : Tensor) -> Tensor: \n",
    "        \"\"\"\n",
    "        Args: \n",
    "            x : Tensor, shape [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        a = self.token_embeddings(x)\n",
    "        \n",
    "        positions = torch.arange(x.size(1)).expand(x.shape[0], -1)\n",
    "        b = self.pos_embeddings(positions)\n",
    "        return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820b4407",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module): \n",
    "    def __init__(self, d_model, dim_ff, nhead, num_encoder_layers, num_decoder_layers, \n",
    "                 src_vocab_size, tgt_vocab_size, src_max_len, tgt_max_len, dropout, \n",
    "                activation=\"gelu\"): \n",
    "        super().__init__()\n",
    "        self.src_embedding = PositionalEmbedding(src_vocab_size, d_model, src_max_len)\n",
    "        self.tgt_embedding = PositionalEmbedding(tgt_vocab_size, d_model, tgt_max_len)\n",
    "        \n",
    "        self.transformer = Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_encoder_layers, \n",
    "                                      num_decoder_layers=num_decoder_layers, dim_feedforward=dim_ff, \n",
    "                                      dropout=dropout, activation=activation, batch_first=True)\n",
    "        \n",
    "        self.lm_head = nn.Linear(d_model, tgt_vocab_size)\n",
    "                \n",
    "        self.nhead = nhead\n",
    "    \n",
    "    def forward(self, src, tgt, src_padding_mask, tgt_padding_mask): \n",
    "        batch_size = src.size(0)\n",
    "        tgt_seq_len = tgt.size(-1)\n",
    "\n",
    "        src_vecs = self.src_embedding(src)\n",
    "        tgt_vecs = self.tgt_embedding(tgt)\n",
    "                \n",
    "        clm_mask = get_clm_mask(self.nhead*batch_size, tgt_seq_len)\n",
    "                \n",
    "        # Note that in pytorch, mask[i,j]=1 means don't attend, so we flip \n",
    "        # the outputs of huggingface tokenizer \n",
    "        x = self.transformer(src=src_vecs, tgt=tgt_vecs, tgt_mask=clm_mask, \n",
    "                             src_key_padding_mask=src_padding_mask==0, \n",
    "                             tgt_key_padding_mask=tgt_padding_mask==0)\n",
    "        \n",
    "        out = self.lm_head(x)\n",
    "        \n",
    "        return out \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e8c35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a quick method for making causal attention masks \n",
    "def clm_mask(batch_size, size): \n",
    "    attn_shape = (1, size, size)\n",
    "    mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return (torch.from_numpy(mask)==1).expand(batch_size, -1, -1)\n",
    "\n",
    "print(\"Causal language modelling mask:\")\n",
    "print(clm_mask(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218ce832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clm_mask(batch_size, seq_len): \n",
    "    attn_shape = (batch_size, seq_len, seq_len)\n",
    "    mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(mask)==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7bc71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make sure everything works\n",
    "toy_model = TransformerModel(d_model=16, dim_ff=64, nhead=2, num_encoder_layers=3, num_decoder_layers=3, \n",
    "                            src_vocab_size=en_tokenizer.vocab_size, tgt_vocab_size=de_tokenizer.vocab_size, \n",
    "                            src_max_len=SRC_SEQ_LEN, tgt_max_len=TGT_SEQ_LEN, dropout=0.1)\n",
    "\n",
    "toy_model.eval()\n",
    "\n",
    "batch = en_tokenizer(en_data[0:5], return_tensors=\"pt\", padding='max_length', max_length=SRC_SEQ_LEN)\n",
    "batch_out = de_tokenizer(de_data[0:5], return_tensors=\"pt\", padding='max_length', max_length=TGT_SEQ_LEN)\n",
    "\n",
    "batch_size = batch['input_ids'].size(0)\n",
    "\n",
    "out = toy_model(src=batch[\"input_ids\"], tgt=batch_out[\"input_ids\"], \n",
    "                    src_padding_mask=batch[\"attention_mask\"], \n",
    "                   tgt_padding_mask=batch_out[\"attention_mask\"])\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863ef4b9",
   "metadata": {},
   "source": [
    "# Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7306ea10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_sqrt_lambda(d_model, num_warmup_steps): \n",
    "    return lambda step: min(math.pow(step+1, -0.5), (step+1) * math.pow((num_warmup_steps+1), -1.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2472a5",
   "metadata": {},
   "source": [
    "### LR scheduler demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa4d0ff",
   "metadata": {},
   "source": [
    "num_warmup_steps = 2000\n",
    "lr = 0.05\n",
    "lrs = []\n",
    "dummy_model = nn.Linear(1,1)\n",
    "optimizer = AdamW(dummy_model.parameters(), lr)\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=inv_sqrt_lambda(512, num_warmup_steps))\n",
    "\n",
    "for _ in range(25_000):\n",
    "    dummy_input = torch.zeros((1, 1))\n",
    "    dummy_loss = dummy_model(dummy_input)\n",
    "    dummy_loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    lrs.append(scheduler.get_last_lr())\n",
    "\n",
    "plt.plot(lrs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5964bbb3",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be833734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model : TransformerModel, eval_data: List[str], src_tokenizer, \n",
    "             tgt_tokenizer, eval_batch_size) -> float: \n",
    "    model.eval()\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum', label_smoothing=0.1)\n",
    "    \n",
    "    total_loss = 0 \n",
    "    loader = DataLoader(eval_data, eval_batch_size, drop_last=False)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        print(\"EVALUATING\")\n",
    "        for srcs, tgts in tqdm(loader):\n",
    "            src_tokens = src_tokenizer(list(srcs), return_tensors=\"pt\", padding=True)\n",
    "            tgt_tokens = tgt_tokenizer(list(tgts), return_tensors=\"pt\", padding=True)\n",
    "            \n",
    "            out = model(src_tokens['input_ids'], tgt_tokens['input_ids'], \n",
    "                       src_tokens['attention_mask'], tgt_tokens['attention_mask'])\n",
    "            \n",
    "            loss = loss_fn(out[:, :-1, :], tgt_tokens['input_ids'])\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "    return total_loss/len(eval_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4c581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: TransformerModel, train_data: List[str], eval_data: List[str], optimizer, scheduler, \n",
    "          num_steps, batch_size, eval_batch_size, src_tokenizer, tgt_tokenizer, write_dir: str, grad_clip=0.5, \n",
    "          log_steps=100, eval_steps=1000): \n",
    "    \n",
    "    writer = SummaryWriter(log_dir=write_dir)\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    \n",
    "    loader = DataLoader(train_data, batch_size=batch_size, drop_last=True, shuffle=True)\n",
    "    \n",
    "    i = 0 \n",
    "    epoch = 0 \n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    print(f\"EPOCH {epoch}, STEP {i}\")\n",
    "    while i<=num_steps:\n",
    "        for srcs, tgts in loader: \n",
    "            src_tokens = src_tokenizer(list(srcs), return_tensors=\"pt\", padding=True)\n",
    "            tgt_tokens = tgt_tokenizer(list(tgts), return_tensors=\"pt\", padding=True)\n",
    "            print(srcs)\n",
    "            print(tgts)\n",
    "            print(src_tokens['input_ids'])\n",
    "            print(tgt_tokens['input_ids'])\n",
    "            print(de_tokenizer.decode(src_tokens['input_ids'].tolist()[0]))\n",
    "            \n",
    "            output = model(src_tokens['input_ids'], tgt_tokens['input_ids'], src_tokens[\"attention_mask\"], \n",
    "                          tgt_tokens[\"attention_mask\"])\n",
    "            \n",
    "            print(output[:, :, :].shape)\n",
    "            \n",
    "            flattened_outs = output[:, :-1, :].view(-1, tgt_tokenizer.vocab_size)\n",
    "            aligned_seqlen = tgt_tokens['input_ids'].size(1) - 1\n",
    "            batch_size = tgt_tokens['input_ids'].size(0)\n",
    "            target = tgt_tokens['input_ids'][:, 1:].view(aligned_seqlen*batch_size)\n",
    "            \n",
    "            loss = loss_fn(flattened_outs, target)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm(model.parameters(), grad_clip)\n",
    "            i+=1 \n",
    "            \n",
    "            if i%log_steps==0: \n",
    "                last_lr = scheduler.get_last_lr()\n",
    "                avg_loss = total_loss/log_steps \n",
    "                ms_per_batch = (time.time()-start_time)*1_000 /log_steps\n",
    "                \n",
    "                writer.add_scalar('lr', last_lr, i)\n",
    "                writer.add_scalar('loss/train', avg_loss, i)\n",
    "                writer.add_scalar('ms/batch', ms_per_batch, i)\n",
    "                \n",
    "                \n",
    "            if i%eval_steps==0: \n",
    "                val_loss = evaluate(model, eval_data, src_tokenizer, tgt_tokenizer, eval_batch_size)\n",
    "                \n",
    "                writer.add_scalar('loss/val', val_loss, i)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e6be46",
   "metadata": {},
   "source": [
    "# A Synthetic Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594536b1",
   "metadata": {},
   "source": [
    "We're going to train our model on a very simple task: the copy task, where we have an alphabet of 5 characters (!, @, #, $, ^), and the model's job is to copy the source sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42441d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choices, randrange\n",
    "def data_gen(): \n",
    "    alphabet = ['!', '@', '#', '$', '^']\n",
    "    copy = []\n",
    "    for _ in range(10_000): \n",
    "        k = randrange(1, 15)\n",
    "        seq = \"\".join(choices(alphabet, k=k))\n",
    "        copy.append((seq, de_tokenizer.bos_token+seq+de_tokenizer.eos_token))\n",
    "    return copy\n",
    "\n",
    "copy = data_gen()\n",
    "\n",
    "copy_train = copy[:8000]\n",
    "copy_val = copy[8000:9000]\n",
    "copy_test = copy[9000:]\n",
    "\n",
    "for pair in copy_train[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83185f9f",
   "metadata": {},
   "source": [
    "Now we train the simple model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbc9121",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_MODEL = 16\n",
    "toy_model = TransformerModel(d_model=D_MODEL, dim_ff=64, nhead=2, num_encoder_layers=3, num_decoder_layers=3, \n",
    "                            src_vocab_size=en_tokenizer.vocab_size, tgt_vocab_size=de_tokenizer.vocab_size, \n",
    "                            src_max_len=15, tgt_max_len=17, dropout=0.1)\n",
    "\n",
    "optimizer = AdamW(toy_model.parameters(), lr=1/math.sqrt(D_MODEL))\n",
    "num_warmup_steps=100 \n",
    "scheduler = LambdaLR(optimizer, lr_lambda=inv_sqrt_lambda(D_MODEL, num_warmup_steps))\n",
    "\n",
    "train(model=toy_model, train_data=copy_train, eval_data=copy_val, optimizer=optimizer, scheduler=scheduler, \n",
    "     num_steps=1000, batch_size=4, eval_batch_size=50, src_tokenizer=de_tokenizer, tgt_tokenizer=de_tokenizer, \n",
    "     write_dir = \"runs/test1\", log_steps=100, eval_steps=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
