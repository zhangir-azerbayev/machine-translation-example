{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efd76cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "import json \n",
    "\n",
    "import math \n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "import numpy as np \n",
    "\n",
    "import torch \n",
    "from torch import nn, Tensor\n",
    "from torch.nn import Linear, Transformer\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2071bcee",
   "metadata": {},
   "source": [
    "# Data: EN-DE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbeda70",
   "metadata": {},
   "source": [
    "First, let's get a feel for the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df90f439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizer(name_or_path='bert-base-cased', vocab_size=28996, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "PreTrainedTokenizer(name_or_path='bert-base-german-cased', vocab_size=30000, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[BOS]', 'eos_token': '[EOS]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "# Using this for convenience just for now\n",
    "en_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "de_tokenizer = BertTokenizer.from_pretrained(\"bert-base-german-cased\")\n",
    "de_tokenizer.bos_token = \"[BOS]\"\n",
    "de_tokenizer.eos_token = \"[EOS]\"\n",
    "print(en_tokenizer)\n",
    "print(de_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84f7c6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/raw_data/mmt_wmt17_train.en\") as f: \n",
    "    en_data = [x.strip() for x in f.readlines()]\n",
    "\n",
    "with open(\"data/raw_data/mmt_wmt17_train.de\") as f: \n",
    "    # we're going to use sep as bos and \n",
    "    de_data = [de_tokenizer.bos_token + x.strip() + de_tokenizer.eos_token for x in f.readlines()]\n",
    "    \n",
    "en_de_data = [(x, y) for x,y in zip(en_data, de_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ad0f065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Two young, White males are outside near many bushes.', '[BOS]Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.[EOS]')\n",
      "('Several men in hard hats are operating a giant pulley system.', '[BOS]Mehrere Männer mit Schutzhelmen bedienen ein Antriebsradsystem.[EOS]')\n",
      "('A little girl climbing into a wooden playhouse.', '[BOS]Ein kleines Mädchen klettert in ein Spielhaus aus Holz.[EOS]')\n",
      "('A man in a blue shirt is standing on a ladder cleaning a window.', '[BOS]Ein Mann in einem blauen Hemd steht auf einer Leiter und putzt ein Fenster.[EOS]')\n",
      "('Two men are at the stove preparing food.', '[BOS]Zwei Männer stehen am Herd und bereiten Essen zu.[EOS]')\n",
      "('A man in green holds a guitar while the other man observes his shirt.', '[BOS]Ein Mann in grün hält eine Gitarre, während der andere Mann sein Hemd ansieht.[EOS]')\n",
      "('A man is smiling at a stuffed lion', '[BOS]Ein Mann lächelt einen ausgestopften Löwen an.[EOS]')\n",
      "('A trendy girl talking on her cellphone while gliding slowly down the street.', '[BOS]Ein schickes Mädchen spricht mit dem Handy während sie langsam die Straße entlangschwebt.[EOS]')\n",
      "('A woman with a large purse is walking by a gate.', '[BOS]Eine Frau mit einer großen Geldbörse geht an einem Tor vorbei.[EOS]')\n",
      "('Boys dancing on poles in the middle of the night.', '[BOS]Jungen tanzen mitten in der Nacht auf Pfosten.[EOS]')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(x) for x in en_de_data[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bd1e21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Two young, White males are outside near many bushes.', 'Several men in hard hats are operating a giant pulley system.', 'A little girl climbing into a wooden playhouse.'), ('[BOS]Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.[EOS]', '[BOS]Mehrere Männer mit Schutzhelmen bedienen ein Antriebsradsystem.[EOS]', '[BOS]Ein kleines Mädchen klettert in ein Spielhaus aus Holz.[EOS]')]\n",
      "[('A man in a blue shirt is standing on a ladder cleaning a window.', 'Two men are at the stove preparing food.'), ('[BOS]Ein Mann in einem blauen Hemd steht auf einer Leiter und putzt ein Fenster.[EOS]', '[BOS]Zwei Männer stehen am Herd und bereiten Essen zu.[EOS]')]\n"
     ]
    }
   ],
   "source": [
    "# We'll be using torch.utils.DataLoader a lot \n",
    "loader = DataLoader(en_de_data[:5], batch_size=3)\n",
    "\n",
    "for x in loader:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95036f49",
   "metadata": {},
   "source": [
    "en_lens = [len(x) for x in en_tokenizer(en_data)['input_ids']]\n",
    "plt.hist(en_lens)\n",
    "plt.title(\"Lengths of input sequences\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df16bd5c",
   "metadata": {},
   "source": [
    "de_lens = [len(x) for x in de_tokenizer(de_data)['input_ids']]\n",
    "plt.hist(de_lens)\n",
    "plt.title(\"Lengths of output sequences\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c8ee0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on our data, we declare the following parameters: \n",
    "\n",
    "SRC_SEQ_LEN = 55\n",
    "TGT_SEQ_LEN = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fb7d5e",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f4f584",
   "metadata": {},
   "source": [
    "First, let's define how we're doing positional encodings. We're going to use learned positional encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f39afb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module): \n",
    "    def __init__(self, vocab_size, d_embedding, max_seq_len): \n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, d_embedding)\n",
    "        self.pos_embeddings = nn.Embedding(max_seq_len, d_embedding)\n",
    "    \n",
    "    def forward(self, x : Tensor) -> Tensor: \n",
    "        \"\"\"\n",
    "        Args: \n",
    "            x : Tensor, shape [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        a = self.token_embeddings(x)\n",
    "        \n",
    "        positions = torch.arange(x.size(1)).expand(x.shape[0], -1)\n",
    "        b = self.pos_embeddings(positions)\n",
    "        return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "820b4407",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module): \n",
    "    def __init__(self, d_model, dim_ff, nhead, num_encoder_layers, num_decoder_layers, \n",
    "                 src_vocab_size, tgt_vocab_size, src_max_len, tgt_max_len, dropout, \n",
    "                activation=\"gelu\"): \n",
    "        super().__init__()\n",
    "        self.src_embedding = PositionalEmbedding(src_vocab_size, d_model, src_max_len)\n",
    "        self.tgt_embedding = PositionalEmbedding(tgt_vocab_size, d_model, tgt_max_len)\n",
    "        \n",
    "        self.transformer = Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_encoder_layers, \n",
    "                                      num_decoder_layers=num_decoder_layers, dim_feedforward=dim_ff, \n",
    "                                      dropout=dropout, activation=activation, batch_first=True)\n",
    "        \n",
    "        self.lm_head = nn.Linear(d_model, tgt_vocab_size)\n",
    "                \n",
    "        self.nhead = nhead\n",
    "    \n",
    "    def forward(self, src, tgt, src_padding_mask, tgt_padding_mask): \n",
    "        batch_size = src.size(0)\n",
    "        tgt_seq_len = tgt.size(-1)\n",
    "\n",
    "        src_vecs = self.src_embedding(src)\n",
    "        tgt_vecs = self.tgt_embedding(tgt)\n",
    "                \n",
    "        clm_mask = get_clm_mask(self.nhead*batch_size, tgt_seq_len)\n",
    "                \n",
    "        # Note that in pytorch, mask[i,j]=1 means don't attend, so we flip \n",
    "        # the outputs of huggingface tokenizer \n",
    "        x = self.transformer(src=src_vecs, tgt=tgt_vecs, tgt_mask=clm_mask, \n",
    "                             src_key_padding_mask=src_padding_mask==0, \n",
    "                             tgt_key_padding_mask=tgt_padding_mask==0)\n",
    "        \n",
    "        out = self.lm_head(x)\n",
    "        \n",
    "        return out \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48e8c35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal language modelling mask:\n",
      "tensor([[[False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False, False, False,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False, False, False, False,  True,  True,  True,  True,  True,  True],\n",
      "         [False, False, False, False, False,  True,  True,  True,  True,  True],\n",
      "         [False, False, False, False, False, False,  True,  True,  True,  True],\n",
      "         [False, False, False, False, False, False, False,  True,  True,  True],\n",
      "         [False, False, False, False, False, False, False, False,  True,  True],\n",
      "         [False, False, False, False, False, False, False, False, False,  True],\n",
      "         [False, False, False, False, False, False, False, False, False, False]]])\n"
     ]
    }
   ],
   "source": [
    "# This is a quick method for making causal attention masks \n",
    "def clm_mask(batch_size, size): \n",
    "    attn_shape = (1, size, size)\n",
    "    mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return (torch.from_numpy(mask)==1).expand(batch_size, -1, -1)\n",
    "\n",
    "print(\"Causal language modelling mask:\")\n",
    "print(clm_mask(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6523c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clm_mask(batch_size, seq_len): \n",
    "    attn_shape = (batch_size, seq_len, seq_len)\n",
    "    mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(mask)==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e7bc71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 50, 30000])\n"
     ]
    }
   ],
   "source": [
    "# Let's make sure everything works\n",
    "toy_model = TransformerModel(d_model=16, dim_ff=64, nhead=2, num_encoder_layers=3, num_decoder_layers=3, \n",
    "                            src_vocab_size=en_tokenizer.vocab_size, tgt_vocab_size=de_tokenizer.vocab_size, \n",
    "                            src_max_len=SRC_SEQ_LEN, tgt_max_len=TGT_SEQ_LEN, dropout=0.1)\n",
    "\n",
    "toy_model.eval()\n",
    "\n",
    "batch = en_tokenizer(en_data[0:5], return_tensors=\"pt\", padding='max_length', max_length=SRC_SEQ_LEN)\n",
    "batch_out = de_tokenizer(de_data[0:5], return_tensors=\"pt\", padding='max_length', max_length=TGT_SEQ_LEN)\n",
    "\n",
    "batch_size = batch['input_ids'].size(0)\n",
    "\n",
    "out = toy_model(src=batch[\"input_ids\"], tgt=batch_out[\"input_ids\"], \n",
    "                    src_padding_mask=batch[\"attention_mask\"], \n",
    "                   tgt_padding_mask=batch_out[\"attention_mask\"])\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0859b530",
   "metadata": {},
   "source": [
    "# Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1456fb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_sqrt_lambda(d_model, num_warmup_steps): \n",
    "    return lambda step: min(math.pow(step+1, -0.5), (step+1) * math.pow((num_warmup_steps+1), -1.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9c0814",
   "metadata": {},
   "source": [
    "### LR scheduler demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4852015",
   "metadata": {},
   "source": [
    "num_warmup_steps = 2000\n",
    "lr = 0.05\n",
    "lrs = []\n",
    "dummy_model = nn.Linear(1,1)\n",
    "optimizer = AdamW(dummy_model.parameters(), lr)\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=inv_sqrt_lambda(512, num_warmup_steps))\n",
    "\n",
    "for _ in range(25_000):\n",
    "    dummy_input = torch.zeros((1, 1))\n",
    "    dummy_loss = dummy_model(dummy_input)\n",
    "    dummy_loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    lrs.append(scheduler.get_last_lr())\n",
    "\n",
    "plt.plot(lrs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a496e6",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48adfdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model : TransformerModel, eval_data: List[str], src_tokenizer, \n",
    "             tgt_tokenizer, eval_batch_size) -> float: \n",
    "    model.eval()\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum', label_smoothing=0.1)\n",
    "    \n",
    "    total_loss = 0 \n",
    "    loader = DataLoader(eval_data, eval_batch_size, drop_last=False)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        print(\"EVALUATING\")\n",
    "        for srcs, tgts in tqdm(loader):\n",
    "            src_tokens = src_tokenizer(list(srcs), return_tensors=\"pt\", padding=True)\n",
    "            tgt_tokens = tgt_tokenizer(list(tgts), return_tensors=\"pt\", padding=True)\n",
    "            \n",
    "            out = model(src_tokens['input_ids'], tgt_tokens['input_ids'], \n",
    "                       src_tokens['attention_mask'], tgt_tokens['attention_mask'])\n",
    "            \n",
    "            loss = loss_fn(out[:, :-1, :], tgt_tokens['input_ids'])\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "    return total_loss/len(eval_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bd53750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: TransformerModel, train_data: List[str], eval_data: List[str], optimizer, scheduler, \n",
    "          num_steps, batch_size, eval_batch_size, src_tokenizer, tgt_tokenizer, write_dir: str, grad_clip=0.5, \n",
    "          log_steps=100, eval_steps=1000): \n",
    "    \n",
    "    writer = SummaryWriter(log_dir=write_dir)\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    \n",
    "    loader = DataLoader(train_data, batch_size=batch_size, drop_last=True, shuffle=True)\n",
    "    \n",
    "    i = 0 \n",
    "    epoch = 0 \n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    print(f\"EPOCH {epoch}, STEP {i}\")\n",
    "    while i<=num_steps:\n",
    "        for srcs, tgts in loader: \n",
    "            src_tokens = src_tokenizer(list(srcs), return_tensors=\"pt\", padding=True)\n",
    "            tgt_tokens = tgt_tokenizer(list(tgts), return_tensors=\"pt\", padding=True)\n",
    "            print(srcs)\n",
    "            print(tgts)\n",
    "            print(src_tokens['input_ids'])\n",
    "            print(tgt_tokens['input_ids'])\n",
    "            print(de_tokenizer.decode(src_tokens['input_ids'].tolist()[0]))\n",
    "            \n",
    "            output = model(src_tokens['input_ids'], tgt_tokens['input_ids'], src_tokens[\"attention_mask\"], \n",
    "                          tgt_tokens[\"attention_mask\"])\n",
    "            \n",
    "            print(output[:, :, :].shape)\n",
    "            \n",
    "            flattened_outs = output[:, :-1, :].view(-1, tgt_tokenizer.vocab_size)\n",
    "            aligned_seqlen = tgt_tokens['input_ids'].size(1) - 1\n",
    "            batch_size = tgt_tokens['input_ids'].size(0)\n",
    "            target = tgt_tokens['input_ids'][:, 1:].view(aligned_seqlen*batch_size)\n",
    "            \n",
    "            loss = loss_fn(flattened_outs, target)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm(model.parameters(), grad_clip)\n",
    "            i+=1 \n",
    "            \n",
    "            if i%log_steps==0: \n",
    "                last_lr = scheduler.get_last_lr()\n",
    "                avg_loss = total_loss/log_steps \n",
    "                ms_per_batch = (time.time()-start_time)*1_000 /log_steps\n",
    "                \n",
    "                writer.add_scalar('lr', last_lr, i)\n",
    "                writer.add_scalar('loss/train', avg_loss, i)\n",
    "                writer.add_scalar('ms/batch', ms_per_batch, i)\n",
    "                \n",
    "                \n",
    "            if i%eval_steps==0: \n",
    "                val_loss = evaluate(model, eval_data, src_tokenizer, tgt_tokenizer, eval_batch_size)\n",
    "                \n",
    "                writer.add_scalar('loss/val', val_loss, i)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e6be46",
   "metadata": {},
   "source": [
    "# A Synthetic Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594536b1",
   "metadata": {},
   "source": [
    "We're going to train our model on a very simple task: the copy task, where we have an alphabet of 5 characters (!, @, #, $, ^), and the model's job is to copy the source sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42441d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('$', '[BOS]$[EOS]')\n",
      "('@', '[BOS]@[EOS]')\n",
      "('$', '[BOS]$[EOS]')\n",
      "('@^!^@#^', '[BOS]@^!^@#^[EOS]')\n",
      "('!^@^#@', '[BOS]!^@^#@[EOS]')\n",
      "('!^#@!^!###!', '[BOS]!^#@!^!###![EOS]')\n",
      "('^!#!', '[BOS]^!#![EOS]')\n",
      "('#$$#', '[BOS]#$$#[EOS]')\n",
      "('#$!^$#!#@', '[BOS]#$!^$#!#@[EOS]')\n",
      "('#!$$$^^^$!!!@^', '[BOS]#!$$$^^^$!!!@^[EOS]')\n"
     ]
    }
   ],
   "source": [
    "from random import choices, randrange\n",
    "def data_gen(): \n",
    "    alphabet = ['!', '@', '#', '$', '^']\n",
    "    copy = []\n",
    "    for _ in range(10_000): \n",
    "        k = randrange(1, 15)\n",
    "        seq = \"\".join(choices(alphabet, k=k))\n",
    "        copy.append((seq, de_tokenizer.bos_token+seq+de_tokenizer.eos_token))\n",
    "    return copy\n",
    "\n",
    "copy = data_gen()\n",
    "\n",
    "copy_train = copy[:8000]\n",
    "copy_val = copy[8000:9000]\n",
    "copy_test = copy[9000:]\n",
    "\n",
    "for pair in copy_train[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83185f9f",
   "metadata": {},
   "source": [
    "Now we train the simple model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61d12eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0, STEP 0\n",
      "('!$^!^^', '!###^#!^!$!@', '^^#@', '!!$!#$$$##')\n",
      "('[BOS]!$^!^^[EOS]', '[BOS]!###^#!^!$!@[EOS]', '[BOS]^^#@[EOS]', '[BOS]!!$!#$$$##[EOS]')\n",
      "tensor([[    3, 26982, 26992, 26999, 26982, 26999, 26999,     4,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [    3, 26982, 26990, 26990, 26990, 26999, 26990, 26982, 26999, 26982,\n",
      "         26992, 26982, 26991,     4],\n",
      "        [    3, 26999, 26999, 26990, 26991,     4,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [    3, 26982, 26982, 26992, 26982, 26990, 26992, 26992, 26992, 26990,\n",
      "         26990,     4,     0,     0]])\n",
      "tensor([[    3, 26984,    35, 10053, 26985, 26982, 26992, 26999, 26982, 26999,\n",
      "         26999, 26984,    55, 10053, 26985,     4,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [    3, 26984,    35, 10053, 26985, 26982, 26990, 26990, 26990, 26999,\n",
      "         26990, 26982, 26999, 26982, 26992, 26982, 26991, 26984,    55, 10053,\n",
      "         26985,     4],\n",
      "        [    3, 26984,    35, 10053, 26985, 26999, 26999, 26990, 26991, 26984,\n",
      "            55, 10053, 26985,     4,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [    3, 26984,    35, 10053, 26985, 26982, 26982, 26992, 26982, 26990,\n",
      "         26992, 26992, 26992, 26990, 26990, 26984,    55, 10053, 26985,     4,\n",
      "             0,     0]])\n",
      "[CLS]! $ ^! ^ ^ [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m num_warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m \n\u001b[1;32m      8\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m LambdaLR(optimizer, lr_lambda\u001b[38;5;241m=\u001b[39minv_sqrt_lambda(D_MODEL, num_warmup_steps))\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoy_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m     \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_tokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mde_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_tokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mde_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m     \u001b[49m\u001b[43mwrite_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mruns/test1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_data, eval_data, optimizer, scheduler, num_steps, batch_size, eval_batch_size, src_tokenizer, tgt_tokenizer, write_dir, grad_clip, log_steps, eval_steps)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(tgt_tokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(de_tokenizer\u001b[38;5;241m.\u001b[39mdecode(src_tokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m---> 26\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m              \u001b[49m\u001b[43mtgt_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(output[:, :, :]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     31\u001b[0m flattened_outs \u001b[38;5;241m=\u001b[39m output[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, tgt_tokenizer\u001b[38;5;241m.\u001b[39mvocab_size)\n",
      "File \u001b[0;32m~/anaconda3/envs/proofnet/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, src, tgt, src_padding_mask, tgt_padding_mask)\u001b[0m\n\u001b[1;32m     19\u001b[0m tgt_seq_len \u001b[38;5;241m=\u001b[39m tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     21\u001b[0m src_vecs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_embedding(src)\n\u001b[0;32m---> 22\u001b[0m tgt_vecs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtgt_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m clm_mask \u001b[38;5;241m=\u001b[39m get_clm_mask(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnhead\u001b[38;5;241m*\u001b[39mbatch_size, tgt_seq_len)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Note that in pytorch, mask[i,j]=1 means don't attend, so we flip \u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# the outputs of huggingface tokenizer \u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/proofnet/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mPositionalEmbedding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embeddings(x)\n\u001b[1;32m     15\u001b[0m positions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mexpand(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a \u001b[38;5;241m+\u001b[39m b\n",
      "File \u001b[0;32m~/anaconda3/envs/proofnet/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/proofnet/lib/python3.10/site-packages/torch/nn/modules/sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/proofnet/lib/python3.10/site-packages/torch/nn/functional.py:2199\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2193\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2195\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2196\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2197\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2198\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "D_MODEL = 16\n",
    "toy_model = TransformerModel(d_model=D_MODEL, dim_ff=64, nhead=2, num_encoder_layers=3, num_decoder_layers=3, \n",
    "                            src_vocab_size=en_tokenizer.vocab_size, tgt_vocab_size=de_tokenizer.vocab_size, \n",
    "                            src_max_len=15, tgt_max_len=17, dropout=0.1)\n",
    "\n",
    "optimizer = AdamW(toy_model.parameters(), lr=1/math.sqrt(D_MODEL))\n",
    "num_warmup_steps=100 \n",
    "scheduler = LambdaLR(optimizer, lr_lambda=inv_sqrt_lambda(D_MODEL, num_warmup_steps))\n",
    "\n",
    "train(model=toy_model, train_data=copy_train, eval_data=copy_val, optimizer=optimizer, scheduler=scheduler, \n",
    "     num_steps=1000, batch_size=4, eval_batch_size=50, src_tokenizer=de_tokenizer, tgt_tokenizer=de_tokenizer, \n",
    "     write_dir = \"runs/test1\", log_steps=100, eval_steps=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
